from airflow import DAG
from airflow.providers.amazon.aws.operators.redshift_sql import RedshiftSQLOperator
from airflow.providers.amazon.aws.transfers.redshift_to_s3 import RedshiftToS3Operator
from airflow.models import Variable
from airflow.providers.postgres.operators.postgres import PostgresOperator

from datetime import datetime, timedelta


default_args = {
    'retries': 1,
    'retry_delay': timedelta(minutes=2)
}

with DAG(
    dag_id="DM_{{ dag_id }}_To_RDS",
    default_args = default_args ,
    start_date=datetime(2022, 8, 17),
    catchup=False,
    schedule_interval=None
)as dag:

    s3_folder = "DM/"
    
    # DM table - transform 
    create_dm = RedshiftSQLOperator(
        task_id='create_dm_{{ table }}',
        redshift_conn_id = 'redshift_dev_db',
        autocommit = True,
        sql = []
    )
    
    
    
    ### Transfer DM table from redshift to s3
    
    dm_to_s3 = RedshiftToS3Operator(
        task_id="transfer_redshift_to_s3",
        redshift_conn_id = 'redshift_dev_db',
        s3_bucket = Variable.get('S3-BUCKET'),
        s3_key=s3_folder+"{{ table }}/"+s3_key,
        schema="{{ schema }}",
        table="{{ table }}",
        select_query="SELECT * FROM {{ schema }}.{{ table }};",
        unload_options = ['FORMAT CSV','PARALLEL OFF', 'EXTENSION \'csv\'', 'ALLOWOVERWRITE'],
        table_as_file_name = True,
    )
    
    # file_sensor = S3KeySensor(
    #     task_id='s3_key_sensor'
    # ) 
    
    # Check if table exists
    check_created_table = PostgresOperator(
        task_id = "check_create_table",
        postgres_conn_id = 'postgres_default',
        autocommit = True,
        sql = ["DROP TABLE IF EXISTS {{ table }};"]
        
    )
    
    # Import s3 from rds
    s3_to_rds = PostgresOperator(
        task_id = "s3_to_rds",
        postgres_conn_id = 'postgres_default',
        autocommit = True,
        sql = f""" 
        SELECT aws_s3.table_import_from_s3(
            '{{ table }}',
            '',
            '(format csv, HEADER TRUE)',
            '{Variable.get('S3-BUCKET')}',
            '{s3_folder}{{ table }}/{s3_key}/{{table}}_000.csv',
            '{Variable.get('AWS_DEFAULT_REGION')}'
        );"""
    )

        
        
    create_dm >> dm_to_s3 >> check_created_table >> s3_to_rds
        
        
    
    
    